#Creating compute cluster

from azureml.core.compute_target import ComputeTargetException
cpu_cluster_name = "arvc-cpu-cluster"

# Verify that cluster does not exist already
try:
    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)
    print('Found existing cluster, use it.')
except ComputeTargetException:
    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)
    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)

cpu_cluster.wait_for_completion(show_output=True)


----------------------
# Specify parameter sampler
ps = RandomParameterSampling( {
        "--C": uniform(0.00, 2.5),
        "--max_iter": randint(250)
    }
)

# Specify a Policy
policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)

if "training" not in os.listdir():
    os.mkdir("./training")

# Create a SKLearn estimator for use with train.py
est = SKLearn(source_directory=".", compute_target=cpu_cluster, entry_script='train.py')

# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.
hyperdrive_config = HyperDriveConfig(estimator=est, hyperparameter_sampling=ps, policy=policy, primary_metric_name="Accuracy", 
primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, max_total_runs=50, max_concurrent_runs=10, max_duration_minutes=30)


------------------
hdr = exp.submit(config=hyperdrive_config)

RunDetails(hdr).show()
-------------------

best_run = hdr.get_best_run_by_primary_metric()

if best_run is None: 
    raise Exception("No best run was found")

parameter_values = best_run.get_details()['runDefinition']['arguments']

print(parameter_values)

os.makedirs("best_run_model_files", exist_ok=True)

for f in best_run.get_file_names():
    if f == "outputs/model.pkl":
        best_run.download_file(name=f, output_file_path = os.path.join("best_run_model_files", os.path.basename(f)))









-------------Auto ML------------------------
ds = TabularDatasetFactory.from_delimited_files("https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv",validate=True, include_path=False, infer_column_types=True, set_column_types=None, separator=',', header=True, partition_format=None, support_multi_line=False, empty_as_string=False)


-----------------------------------------
x, y = clean_data(ds)

import pandas as pd
df = x.merge(y.rename('y'), left_index=True, right_index=True)
os.makedirs("data", exist_ok=True)
local_path = "data/prepared.csv"
df.to_csv(local_path)

datastore = ws.get_default_datastore()
datastore.upload(src_dir="data", target_path="data")

from azureml.core import Dataset
tabulardataset = Dataset.Tabular.from_delimited_files(path=[(datastore, ("data/prepared.csv"))])


--------------
automl_config = AutoMLConfig(
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='AUC_weighted',
    training_data=tabulardataset,
    label_column_name='y',
    compute_target = cpu_cluster,
    n_cross_validations=10,
    max_concurrent_iterations=25)
    
   
   
-------------------------
automl_run = exp.submit(automl_config, show_output=False)


-------------------------------
best_run, fitted_model = automl_run.get_output()
print(best_run)
print(fitted_model)































