#Creating compute cluster

from azureml.core.compute_target import ComputeTargetException
cpu_cluster_name = "arvc-cpu-cluster"

# Verify that cluster does not exist already
try:
    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)
    print('Found existing cluster, use it.')
except ComputeTargetException:
    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', max_nodes=4)
    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)

cpu_cluster.wait_for_completion(show_output=True)


----------------------
# Specify parameter sampler
ps = RandomParameterSampling( {
        "--C": uniform(0.00, 2.5),
        "--max_iter": randint(250)
    }
)

# Specify a Policy
policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)

if "training" not in os.listdir():
    os.mkdir("./training")

# Create a SKLearn estimator for use with train.py
est = SKLearn(source_directory=".", compute_target=cpu_cluster, entry_script='train.py')

# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.
hyperdrive_config = HyperDriveConfig(estimator=est, hyperparameter_sampling=ps, policy=policy, primary_metric_name="Accuracy", 
primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, max_total_runs=50, max_concurrent_runs=10, max_duration_minutes=30)


------------------
hdr = exp.submit(config=hyperdrive_config)

RunDetails(hdr).show()
-------------------

best_run = hdr.get_best_run_by_primary_metric()

if best_run is None: 
    raise Exception("No best run was found")

parameter_values = best_run.get_details()['runDefinition']['arguments']

print(parameter_values)

os.makedirs("best_run_model_files", exist_ok=True)

for f in best_run.get_file_names():
    if f == "outputs/model.pkl":
        best_run.download_file(name=f, output_file_path = os.path.join("best_run_model_files", os.path.basename(f)))









-------------Auto ML------------------------
ds = TabularDatasetFactory.from_delimited_files("https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv",validate=True, include_path=False, infer_column_types=True, set_column_types=None, separator=',', header=True, partition_format=None, support_multi_line=False, empty_as_string=False)


-----------------------------------------
x, y = clean_data(ds)

import pandas as pd
df = x.merge(y.rename('y'), left_index=True, right_index=True)
os.makedirs("data", exist_ok=True)
local_path = "data/prepared.csv"
df.to_csv(local_path)

datastore = ws.get_default_datastore()
datastore.upload(src_dir="data", target_path="data")

from azureml.core import Dataset
tabulardataset = Dataset.Tabular.from_delimited_files(path=[(datastore, ("data/prepared.csv"))])


--------------
automl_config = AutoMLConfig(
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='AUC_weighted',
    training_data=tabulardataset,
    label_column_name='y',
    compute_target = cpu_cluster,
    n_cross_validations=10,
    max_concurrent_iterations=25)
    
   
   
-------------------------
automl_run = exp.submit(automl_config, show_output=False)


-------------------------------
best_run, fitted_model = automl_run.get_output()
print(best_run)
print(fitted_model)




----------------------------
!pip uninstall -y azureml-train-automl-runtime
!pip install  azureml-train-automl-runtime==1.20.0


----------------------
#print the hyperparameters of the best model
from pprint import pprint

def print_model(model, prefix=""):
    for step in model.steps:
        print(prefix + step[0])
        if hasattr(step[1], 'estimators') and hasattr(step[1], 'weights'):
            pprint({'estimators': list(
                e[0] for e in step[1].estimators), 'weights': step[1].weights})
            print()
            for estimator in step[1].estimators:
                print_model(estimator[1], estimator[0] + ' - ')
        else:
            pprint(step[1].get_params())
            print()

print_model(fitted_model)










































datatransformer
{'enable_dnn': None,
 'enable_feature_sweeping': None,
 'feature_sweeping_config': None,
 'feature_sweeping_timeout': None,
 'featurization_config': None,
 'force_text_dnn': None,
 'is_cross_validation': None,
 'is_onnx_compatible': None,
 'logger': None,
 'observer': None,
 'task': None,
 'working_dir': None}

prefittedsoftvotingclassifier
{'estimators': ['0', '31', '23', '19', '32', '30'],
 'weights': [0.4,
             0.3333333333333333,
             0.06666666666666667,
             0.06666666666666667,
             0.06666666666666667,
             0.06666666666666667]}

0 - maxabsscaler
{'copy': True}

0 - lightgbmclassifier
{'boosting_type': 'gbdt',
 'class_weight': None,
 'colsample_bytree': 1.0,
 'importance_type': 'split',
 'learning_rate': 0.1,
 'max_depth': -1,
 'min_child_samples': 20,
 'min_child_weight': 0.001,
 'min_split_gain': 0.0,
 'n_estimators': 100,
 'n_jobs': 1,
 'num_leaves': 31,
 'objective': None,
 'random_state': None,
 'reg_alpha': 0.0,
 'reg_lambda': 0.0,
 'silent': True,
 'subsample': 1.0,
 'subsample_for_bin': 200000,
 'subsample_freq': 0,
 'verbose': -10}

31 - maxabsscaler
{'copy': True}

31 - lightgbmclassifier
{'boosting_type': 'gbdt',
 'class_weight': None,
 'colsample_bytree': 0.2977777777777778,
 'importance_type': 'split',
 'learning_rate': 0.0842121052631579,
 'max_bin': 50,
 'max_depth': -1,
 'min_child_samples': 114,
 'min_child_weight': 8,
 'min_split_gain': 0.8421052631578947,
 'n_estimators': 400,
 'n_jobs': 1,
 'num_leaves': 65,
 'objective': None,
 'random_state': None,
 'reg_alpha': 0.7894736842105263,
 'reg_lambda': 0.7368421052631579,
 'silent': True,
 'subsample': 0.7426315789473684,
 'subsample_for_bin': 200000,
 'subsample_freq': 0,
 'verbose': -10}

23 - standardscalerwrapper
{'class_name': 'StandardScaler',
 'copy': True,
 'module_name': 'sklearn.preprocessing._data',
 'with_mean': False,
 'with_std': False}

23 - xgboostclassifier
{'base_score': 0.5,
 'booster': 'gbtree',
 'colsample_bylevel': 1,
 'colsample_bynode': 1,
 'colsample_bytree': 1,
 'eta': 0.1,
 'gamma': 0,
 'learning_rate': 0.1,
 'max_delta_step': 0,
 'max_depth': 5,
 'max_leaves': 31,
 'min_child_weight': 1,
 'missing': nan,
 'n_estimators': 100,
 'n_jobs': 1,
 'nthread': None,
 'objective': 'reg:logistic',
 'random_state': 0,
 'reg_alpha': 1.25,
 'reg_lambda': 2.0833333333333335,
 'scale_pos_weight': 1,
 'seed': None,
 'silent': None,
 'subsample': 0.8,
 'tree_method': 'auto',
 'verbose': -10,
 'verbosity': 0}

19 - standardscalerwrapper
{'class_name': 'StandardScaler',
 'copy': True,
 'module_name': 'sklearn.preprocessing._data',
 'with_mean': False,
 'with_std': False}

19 - xgboostclassifier
{'base_score': 0.5,
 'booster': 'gbtree',
 'colsample_bylevel': 1,
 'colsample_bynode': 1,
 'colsample_bytree': 1,
 'eta': 0.3,
 'gamma': 0.1,
 'grow_policy': 'lossguide',
 'learning_rate': 0.1,
 'max_bin': 255,
 'max_delta_step': 0,
 'max_depth': 8,
 'max_leaves': 7,
 'min_child_weight': 1,
 'missing': nan,
 'n_estimators': 400,
 'n_jobs': 1,
 'nthread': None,
 'objective': 'reg:logistic',
 'random_state': 0,
 'reg_alpha': 0,
 'reg_lambda': 0.8333333333333334,
 'scale_pos_weight': 1,
 'seed': None,
 'silent': None,
 'subsample': 0.7,
 'tree_method': 'hist',
 'verbose': -10,
 'verbosity': 0}

32 - standardscalerwrapper
{'class_name': 'StandardScaler',
 'copy': True,
 'module_name': 'sklearn.preprocessing._data',
 'with_mean': False,
 'with_std': True}

32 - lightgbmclassifier
{'boosting_type': 'goss',
 'class_weight': None,
 'colsample_bytree': 0.8911111111111111,
 'importance_type': 'split',
 'learning_rate': 0.1,
 'max_bin': 180,
 'max_depth': 10,
 'min_child_samples': 455,
 'min_child_weight': 9,
 'min_split_gain': 0.15789473684210525,
 'n_estimators': 200,
 'n_jobs': 1,
 'num_leaves': 53,
 'objective': None,
 'random_state': None,
 'reg_alpha': 0.42105263157894735,
 'reg_lambda': 0.15789473684210525,
 'silent': True,
 'subsample': 1,
 'subsample_for_bin': 200000,
 'subsample_freq': 0,
 'verbose': -10}

30 - truncatedsvdwrapper
{'n_components': 0.45526315789473687, 'random_state': None}

30 - xgboostclassifier
{'base_score': 0.5,
 'booster': 'gbtree',
 'colsample_bylevel': 1,
 'colsample_bynode': 1,
 'colsample_bytree': 0.8,
 'eta': 0.5,
 'gamma': 0,
 'learning_rate': 0.1,
 'max_delta_step': 0,
 'max_depth': 6,
 'max_leaves': 63,
 'min_child_weight': 1,
 'missing': nan,
 'n_estimators': 10,
 'n_jobs': 1,
 'nthread': None,
 'objective': 'reg:logistic',
 'random_state': 0,
 'reg_alpha': 0,
 'reg_lambda': 2.3958333333333335,
 'scale_pos_weight': 1,
 'seed': None,
 'silent': None,
 'subsample': 0.7,
 'tree_method': 'auto',
 'verbose': -10,
 'verbosity': 0}






