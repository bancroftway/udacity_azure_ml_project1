# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains direct marketing campaign data for a bank. The marketing campaigns were based on phone calls. The dataset contains attributes on the client being called, last contact with client, campaign attributes and a few socio-economic attributes. We are seeking to predict: as a result of the phone call, whether the client signed up for the banking product (term deposit). Therefore, this is a classification problem, with target label 'y' being 'yes' for when the client signed up, and 'no' when they did not. "

**The best performing model was found using Azure AutoML; the model is a VotingEnsemble classifier, and achieved an accuracy of 91.6%**

## Scikit-learn Pipeline
**The pipeline architecture consists of:**
1. Tabular dataset that loads data from a csv file
2. A data cleaning process, which for example, converts categorical features into one-hot encoded features
3. Hyperparameter tuning step which performed random parameter sampling, to finetune 2 hyperparameters of the classifier model: C (inverse of regularization strength) and max_iter (maximum number of iterations for solvers to converge). The hyperparameter tuning used an early stopping strategy
4. The classification algorithm used was scikit learn's Logistic Regression (aka Logit) classifier.

**What are the benefits of the parameter sampler you chose?**
For the parameter C (inverse of regularization strength) the default value is 1.0, so a uniform distribution from 0 to 2.5 was used. This would ensure a fairly wide space from which to pick a random value for the C parameter.
For max_iter parameter, an integral value is required, so a random integer sampling was used, within range of 1 to 250, which would also enable a wide space for random values to be chosen for max_iter parameter.


**What are the benefits of the early stopping policy you chose?**
A bandit policy was used, which terminates any runs where the primary metric is not within the specified slack factor, thereby conserving computing resources

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The best model generated by AutoML is a VotingEnsemble classifier, which predicts based on the weighted average of predicted class probabilities. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
**Model comparison:**
In hyperparameter tuning, a LogisticRegression classifier was used. The best hyperparameter values found were:
C: 1.671915880
max_iter: 56
The best model using the above hyperparameters achieved an accuracy of 0.9074355

In AutoML, the best model was a VotingEnsemble and achieved an accuracy of 0.91572.

The model generated by AutoML was 0.9% better in accuracy than the model from hyperparameter tuning.

**Difference in architecture**
The model used for hyperparameter tuning was a Logistic Regression model, which predicts the probability of a target variable, using a sigmoid function.
The best model identified by AutoML is a VotingEnsemble classifier, which which predicts based on the weighted average of predicted class probabilities. 

**Reasons for differing performance**
1. AutoML attemps to train a large number of models of different algorithms, each iteration itself using a different set of hyperparameters. Since it is not constrained to a model of a single architecture, it is expected that AutoML will generally outperform any single model.
2. AutoML utilizes a variety of feature engineering techniques such as data normalization, dimensionality reduction etc, which contributes to its superior performance


## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Some areas of improvement are:
1. Increasing the training time for both hyperparameter tuning as well as AutoML experiments, which could help identify even better performing models.
2. For hyperparameter tuning, using more powerful algorithms such as ensemble models (StackingClassifier, VotingClassifier) may help produc models that could match the performance of AutoML models.

## Proof of cluster clean up
Cluster is being cleaned up in code, using cluster.delete() method
